# -*- coding: utf-8 -*-
"""NaiveBayes2.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dMKrs4V8fjbHhzHTYYJFV_bKQ23Q61CZ
"""

import numpy as np
import math as math
import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

xtrain = pd.read_csv('/content/drive/MyDrive/x_train.csv')
ytrain = pd.read_csv('/content/drive/MyDrive/y_train.csv')
xtest = pd.read_csv('/content/drive/MyDrive/x_test.csv')
ytest = pd.read_csv('/content/drive/MyDrive/y_test.csv')

#I concatenate the test and training sets to manipulate the data more easily
merged_train = pd.concat([xtrain,ytrain], axis = 1) 
merged_test = pd.concat([xtest,ytest], axis = 1)
data_size = xtrain.shape[0]
vocab_size = xtrain.shape[1]

#Number of spam and ham mails is calculated here
spam_count = ytrain["Prediction"].value_counts()[1]
ham_count = ytrain["Prediction"].value_counts()[0]

#Prior probabilities of having spam and ham mails are calculated here
spam_prior = spam_count/data_size
ham_prior = ham_count/data_size

spam_word_count = merged_train[merged_train['Prediction'] == 1].sum(axis=0).sum() #Number of all the words used in spam mails are calculated here
ham_word_count = merged_train[merged_train['Prediction'] == 0].sum(axis=0).sum() #Number of all the words used in ham mails are calculated here

word_freq = merged_train.groupby(merged_train.columns[-1]).sum() #Number of occurences of a word (given a class) is calculated here for all the words in our vocabulary and stored in dataframe. 
#PRINT HERE HOW THE DATA LOOKS LIKE

#In here I calculated the likelihood of each word for a given class, and I store it in a data frame.
word_freq.iloc[0] = word_freq.iloc[0] / ham_word_count
word_freq.iloc[1] = word_freq.iloc[1] / spam_word_count

#PRINT HERE HOW THE DATA LOOKS LIKE

word_freq = word_freq.replace(0, 10**(-12)) #In order to prevent underflow issues, I replace the zero likelihoods with 10^(-12)


log_frequency = pd.DataFrame(np.log(word_freq.values)) #I take the logarithm of every entry in the likelihood dataframe in order to calculate the lieklihood of feature vector more easily.

#In here I define a function, that takes a mail in the test set and class, and calculates the likelihood of that mail given the class. 
def Likelihood(k,clas):
  likelihood = log_frequency.loc[clas].to_numpy() * xtest.loc[k]

  return likelihood.sum()

#In here I define a function that takes a mail in the test set, and returns a prediction for that mail.
def predict_class(k):
  spam_likelihood = Likelihood(k,1) #Likelihood of the mail given its class is spam
  ham_likelihood = Likelihood(k,0) #Likelihood of the mail given its class is ham
  
  spam_posterior = spam_likelihood + math.log(spam_prior) #Posterior probability of having a spam mail given its feature vector is calculated 
  ham_posterior = ham_likelihood + math.log(ham_prior) #Posterior probability of having a ham mail given its feature vector is calculated 

  if spam_posterior > ham_posterior:
    return 1
  else:
    return 0

test_size = xtest.shape[0]

correct_prediction = 0

for i in range (test_size):
  if predict_class(i) == ytest.loc[i][0]:
    correct_prediction += 1
  
accuracy = correct_prediction / test_size
#Answer for the first question
print("Accuracy is %" , accuracy*100, "Correct prediction is", correct_prediction)
print("test size is", test_size)
print("false prediction is ", test_size - correct_prediction)


# spam is negative, ham is positive
true_negative = 0
true_positive = 0
false_negative = 0
false_positive = 0

for i in range (test_size):
  if predict_class(i) == 1 and ytest.loc[i][0] == 1:
    true_negative += 1
  elif predict_class(i) == 0 and ytest.loc[i][0] == 0:
    true_positive += 1
  elif predict_class(i) == 1 and ytest.loc[i][0] == 0:
    false_negative += 1
  elif predict_class(i) == 0 and ytest.loc[i][0] == 1:
    false_positive += 1

print("True negative is:", true_negative, "True positive is:", true_positive, "False negative is:", false_negative, "False positive:", false_positive )

"""NOW WE'LL ADD **SMOOTHING**"""

word_freq2 = merged_train.groupby(merged_train.columns[-1]).sum()

#Smoothing:
spam_word_count2 = spam_word_count+ 1500

ham_word_count2 = ham_word_count+ 1500

word_freq2 = word_freq2 + 5

#Rest of the code is same with the previous model
word_freq2.iloc[0] = word_freq2.iloc[0] / ham_word_count2
word_freq2.iloc[1] = word_freq2.iloc[1] / spam_word_count2



log_frequency2 = pd.DataFrame(np.log(word_freq2.values))

def Likelihood2(k,clas):
  likelihood = log_frequency2.loc[clas].to_numpy() * xtest.loc[k]

  return likelihood.sum()

def predict_class2(k):
  spam_likelihood = Likelihood2(k,1)
  ham_likelihood = Likelihood2(k,0)
  
  spam_posterior = spam_likelihood + math.log(spam_prior)
  ham_posterior = ham_likelihood + math.log(ham_prior)

  if spam_posterior > ham_posterior:
    return 1
  else:
    return 0

test_size = xtest.shape[0]

correct_prediction2 = 0

for i in range (test_size):
  if predict_class2(i) == ytest.loc[i][0]:
    correct_prediction2 += 1
  
accuracy2 = correct_prediction2 / test_size

print("Accuracy is %" , accuracy2*100, "Correct prediction is", correct_prediction2)
print("test size is", test_size)
print("false prediction is ", test_size - correct_prediction2)


# spam is negative, ham is positive
true_negative2 = 0
true_positive2 = 0
false_negative2 = 0
false_positive2 = 0

for i in range (test_size):
  if predict_class(i) == 1 and ytest.loc[i][0] == 1:
    true_negative2 += 1
  elif predict_class(i) == 0 and ytest.loc[i][0] == 0:
    true_positive2 += 1
  elif predict_class(i) == 1 and ytest.loc[i][0] == 0:
    false_negative2 += 1
  elif predict_class(i) == 0 and ytest.loc[i][0] == 1:
    false_positive2 += 1

print("True negative is:", true_negative2, "True positive is:", true_positive2, "False negative is:", false_negative2, "False positive:", false_positive2)

"""BINOMIAL NAIVE BAYES"""

#In here, for each word, I store the number of spam and ham mails which contain that word 
def count_nonzero(column):
    return (column != 0).sum()

word_freq3 = merged_train.groupby('Prediction').apply(lambda x: x.apply(count_nonzero)).iloc[:,:-1]

#In here I store the likelihood of each words based on binary estimator
word_freq3.iloc[0] = word_freq3.iloc[0]/ham_count
word_freq3.iloc[1] = word_freq3.iloc[1]/spam_count
word_freq3 = word_freq3.replace(0, 10**(-12)) #I replace the 0's with 10^(-12)

#I also store the likelihoods of not having that word given a class, and I replace 0's with 10^(-12)
not_word_freq3 = (1-word_freq3).replace(0, 10**(-12))

#I take the log of each likelihood
log_frequency3 = pd.DataFrame(np.log(word_freq3.values))
log_not_frequency = pd.DataFrame(np.log(not_word_freq3.values))

#I define a function that takes a mail in the test set and takes class, and returns the likelihood of that mail given the class
def Likelihood3(k,clas):
  likelihood = log_frequency3.loc[clas].to_numpy() * (xtest.loc[k] != 0).astype(int) + log_not_frequency.loc[clas].to_numpy() * (xtest.loc[k] == 0).astype(int) 

  return likelihood.sum()

#I define a function that takes a mail in the test set and compares the posterior probabilitiies of mail being spam and ham, returns the maximum one as a prediction 
def predict_class3(k):
  spam_likelihood = Likelihood3(k,1)
  ham_likelihood = Likelihood3(k,0)
  
  spam_posterior = spam_likelihood + math.log(spam_prior) #Posterior proability of being spam given the mail
  ham_posterior = ham_likelihood + math.log(ham_prior)  ##Posterior proability of being ham given the mail

  if spam_posterior > ham_posterior:
    return 1
  else:
    return 0

test_size = xtest.shape[0]

#I count the number of correct predictions
correct_prediction3 = 0

for i in range (test_size):
  if predict_class3(i) == ytest.loc[i][0]:
    correct_prediction3 += 1

#I calculate accuracy and confusion matrix entries 
accuracy3 = correct_prediction3 / test_size

print("Accuracy is %" , accuracy3*100, "Correct prediction is", correct_prediction3)
print("test size is", test_size)
print("false prediction is ", test_size - correct_prediction3)


# spam is negative, ham is positive
true_negative3 = 0
true_positive3 = 0
false_negative3 = 0
false_positive3 = 0

for i in range (test_size):
  if predict_class3(i) == 1 and ytest.loc[i][0] == 1:
    true_negative3 += 1
  elif predict_class3(i) == 0 and ytest.loc[i][0] == 0:
    true_positive3 += 1
  elif predict_class3(i) == 1 and ytest.loc[i][0] == 0:
    false_negative3 += 1
  elif predict_class3(i) == 0 and ytest.loc[i][0] == 1:
    false_positive3 += 1

print("True negative is:", true_negative3, "True positive is:", true_positive3, "False negative is:", false_negative3, "False positive:", false_positive3)

